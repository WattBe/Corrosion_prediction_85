{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIdT9iu_Z4Rb"
   },
   "source": [
    "# Basic regression: Predict Corrosion as Epit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHp3M9ZmrIxj"
   },
   "source": [
    "*This notebook is based on the [tutorial notebook](https://www.tensorflow.org/tutorials/keras/regression) provided by TensorFlow.*\n",
    "\n",
    "This Notebook takes the data that has been cleaned and imputed in book 2. If I have time I would produce another set of the data with the imputation according to each metal plot as file impute Nan Intuitively.\n",
    "\n",
    "This example uses the `tf.keras` API, see [this guide](https://www.tensorflow.org/guide/keras) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1rRo8oNqZ-Rj"
   },
   "outputs": [],
   "source": [
    "import datetime, time, os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score,  mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9xQKvCJ85kCQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/tensorflow/docs\n",
    "    \n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for TensorBoard\n",
    "\n",
    "We will use the TensorBoard to visualize some results. You can find more information and the board itself at the end of this notebook, but we will define the path were the information should be stored directly here at the beginning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this command you can clear any logs from previous runs\n",
    "# If you want to compare different runs you can skip this cell \n",
    "!rm -rf my_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path for new directory \n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "# Define function for creating a new folder for each run\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime('run_%d_%m_%Y-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for using callbacks; \"name\" should be the name of the model you use\n",
    "def get_callbacks(name):\n",
    "    return tf.keras.callbacks.TensorBoard(run_logdir+name, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define seeds etc.\n",
    "RSEED = 42\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_72b0LCNbjx"
   },
   "source": [
    "## The Alloys Data\n",
    "The dataset is available from book 2_Metal Features, already cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "CiX2FI4gZtTt"
   },
   "outputs": [],
   "source": [
    "Alloys= pd.read_csv(\"../data/Alloy_Inter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2oY3pMPagJrO"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cl</th>\n",
       "      <th>Test_Temp</th>\n",
       "      <th>pH</th>\n",
       "      <th>M_Al</th>\n",
       "      <th>M_Fe</th>\n",
       "      <th>M_HEA</th>\n",
       "      <th>M_NiCrMo</th>\n",
       "      <th>M_Other</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Cr</th>\n",
       "      <th>...</th>\n",
       "      <th>Ta</th>\n",
       "      <th>Re</th>\n",
       "      <th>Ce</th>\n",
       "      <th>Ti</th>\n",
       "      <th>Co</th>\n",
       "      <th>B</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Y</th>\n",
       "      <th>Gd</th>\n",
       "      <th>Epit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005133</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Cl  Test_Temp   pH  M_Al  M_Fe  M_HEA  M_NiCrMo  M_Other    Fe    Cr  \\\n",
       "0  0.005133       50.0  7.8   0.0   1.0    0.0       0.0      0.0  69.7  18.0   \n",
       "\n",
       "   ...   Ta   Re   Ce   Ti   Co    B   Mg    Y   Gd   Epit  \n",
       "0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  287.0  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Alloys.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cuym4yvk76vU"
   },
   "source": [
    "### Split the data into train and test\n",
    "\n",
    "Now we'll split the dataset into a training set and a test set.\n",
    "\n",
    "We will use the test set in the final evaluation of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(Alloys, target, random_state=RSEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qn-IGhUE7_1H"
   },
   "outputs": [],
   "source": [
    "Alloys_train = Alloys.sample(frac=0.8, random_state=42)\n",
    "Alloys_test = Alloys.drop(Alloys_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alloys.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gavKO_6DWRMP"
   },
   "source": [
    "Overall statistics, note how each feature covers a very different range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yi2FzC3T21jR"
   },
   "outputs": [],
   "source": [
    "Alloys_test.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Db7Auq1yXUvh"
   },
   "source": [
    "### Split features from labels\n",
    "\n",
    "Before we can start with the modelling process we need to separate our label from the dataset. This label is the value that we will train the model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2sluJdCW7jN"
   },
   "outputs": [],
   "source": [
    "X_train = Alloys_train.copy()\n",
    "X_test =  Alloys_test.copy()\n",
    "\n",
    "y_train = X_train.pop('Epit')\n",
    "y_test = X_test.pop('Epit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRklxK5s388r"
   },
   "source": [
    "## Normalization\n",
    "\n",
    "In the table of statistics it's easy to see how different the ranges of each feature are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcmY6lKKbkw8"
   },
   "outputs": [],
   "source": [
    "X_train.describe().transpose()[['mean', 'std']]\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ywmerQ6dSox"
   },
   "source": [
    "It is good practice to normalize features that use different scales and ranges. In our case we have the metal features, Cl, pH, Test_temp that present no gausian shape, so it is necesary to normalise them rather than standarise them.\n",
    "\n",
    "One reason this is important is because the features are multiplied by the model weights. So the scale of the outputs and the scale of the gradients are affected by the scale of the inputs. \n",
    "\n",
    "Although a model *might* converge without feature normalization, normalization makes training much more stable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFJ6ISropeoo"
   },
   "source": [
    "### The Normalization layer\n",
    "The `preprocessing.Normalization` layer is a clean and simple way to build that preprocessing into your model.\n",
    "\n",
    "The first step is to create the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlC5ooJrgjQF"
   },
   "outputs": [],
   "source": [
    "normalizer = preprocessing.Normalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYA2Ap6nVOha"
   },
   "source": [
    "Then `.adapt()` it to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrBbbjbwV91f"
   },
   "outputs": [],
   "source": [
    "normalizer.adapt(np.array(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZccMR5yV9YV"
   },
   "source": [
    "This calculates the mean and variance, and stores them in the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGn-ukwxSPtx"
   },
   "outputs": [],
   "source": [
    "normalizer.mean.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGWKaF9GSRuN"
   },
   "source": [
    "When the layer is called it returns the input data, with each feature independently normalized. We can have a look at the first training instance and compare the original and normalized features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2l7zFL_XWIRu"
   },
   "outputs": [],
   "source": [
    "first = np.array(X_train[:1])\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print('First example:', first)\n",
    "    print()\n",
    "    print('Normalized:', normalizer(first).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmjdzxKzEu1-"
   },
   "source": [
    "## A DNN regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DT_aHPsrzO1t"
   },
   "source": [
    "The previous section implemented linear models for single and multiple inputs.\n",
    "\n",
    "This section implements single-input and multiple-input DNN models. The code is basically the same except the model is expanded to include some \"hidden\"  non-linear layers. The name \"hidden\" here just means not directly connected to the inputs or outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SWtkIjhrZwa"
   },
   "source": [
    "These models will contain a few more layers than the linear model:\n",
    "\n",
    "1. The normalization layer.\n",
    "2. Two hidden, nonlinear, `Dense` layers using the `relu` nonlinearity.\n",
    "3. A linear single-output layer.\n",
    "\n",
    "Both will use the same training procedure so we'll include the `compile` method in the newly defined `build_and_compile_model` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c26juK7ZG8j-"
   },
   "outputs": [],
   "source": [
    "def build_and_compile_model(norm):\n",
    "    model = keras.Sequential([\n",
    "        norm,\n",
    "        layers.Dense(164, activation='relu', input_dim=28),\n",
    "        layers.Dense(164, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mae',\n",
    "                  metrics='mse',\n",
    "                  optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7T4RP1V36gVn"
   },
   "source": [
    "### One variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvu9gtxTZR5V"
   },
   "source": [
    "Let's start with a DNN model for a single input: \"Fe\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First create the Fe `Normalization` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fe = np.array(X_train['Fe'])\n",
    "\n",
    "Fe_normalizer = preprocessing.Normalization(input_shape = [1,], axis = None)\n",
    "Fe_normalizer.adapt(Fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then we'll build the sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fe_model = tf.keras.Sequential([\n",
    "    Fe_normalizer,\n",
    "    layers.Dense(units=1)])\n",
    "\n",
    "# We can print a summary of the model architecture with the following line:\n",
    "#Fe_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGbPb-PHGbhs"
   },
   "outputs": [],
   "source": [
    "dnn_Fe_model = build_and_compile_model(Fe_normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sj49Og4YGULr"
   },
   "source": [
    "This model has quite a few more trainable parameters than the linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ReAD0n6MsFK-"
   },
   "outputs": [],
   "source": [
    "dnn_Fe_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-qWCsh6DlyH"
   },
   "source": [
    "Let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sD7qHCmNIOY0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = dnn_Fe_model.fit(\n",
    "    X_train['Fe'], y_train,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=100,\n",
    "    callbacks=get_callbacks(\"dnn_Fe\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dArGGxHxcKjN"
   },
   "source": [
    "This model does slightly better than the linear-Fe model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcF6UWjdCU8T"
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG1snlpR2QCK"
   },
   "source": [
    "If you plot the predictions as a function of `Fe`, you'll see how this model takes advantage of the nonlinearity provided by the hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPF53Rem14NS"
   },
   "outputs": [],
   "source": [
    "x = tf.linspace(0.0, 250, 251)\n",
    "y = dnn_Fe_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsf9rD8I17Wq"
   },
   "outputs": [],
   "source": [
    "plot_Fe(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxCJKIUpe4io"
   },
   "source": [
    "We'll also collect the results on the test set, for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJjM0dU52XtN"
   },
   "outputs": [],
   "source": [
    "test_results['dnn_Fe_model'] = dnn_Fe_model.evaluate(\n",
    "    X_train['Fe'], y_train,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_2Btebp2e64"
   },
   "source": [
    "### Full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKFtezDldLSf"
   },
   "source": [
    "If you repeat this process using all the inputs it slightly improves the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0mhscXh2k36"
   },
   "outputs": [],
   "source": [
    "dnn_model = build_and_compile_model(normalizer)\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXDENACl2tuW"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = dnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=100,\n",
    "    callbacks=get_callbacks(\"dnn_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9Dbj0fX23RQ"
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWoVYS34fJPZ"
   },
   "source": [
    "Collect the results on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bZIa96W3c7K"
   },
   "outputs": [],
   "source": [
    "test_results['dnn_model'] = dnn_model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiCucdPLfMkZ"
   },
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDf1xebEfWBw"
   },
   "source": [
    "Now that all the models are trained let's check the test-set performance and compare how they performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5_ooufM5iH2"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(test_results, index=['Mean absolute error [Epit]', 'Root Mean squared error [Epit]', 'R2[Epit]', 'Ratio[Epit]']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DABIVzsCf-QI"
   },
   "source": [
    "These results match the validation error seen during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft603OzXuEZC"
   },
   "source": [
    "### Make predictions\n",
    "\n",
    "Finally, predict have a look at the errors made by the model when making predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xe7RXH3N3CWU"
   },
   "outputs": [],
   "source": [
    "y_pred = dnn_model.predict(X_test).flatten()\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('True Values [Epit]')\n",
    "plt.ylabel('Predictions [Epit]')\n",
    "lims = [-1000,  1800]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19wyogbOSU5t"
   },
   "source": [
    "It looks like the model predicts reasonably well. \n",
    "\n",
    "Now take a look at the error distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-OHX4DiXd8x"
   },
   "outputs": [],
   "source": [
    "error = y_pred - y_test\n",
    "plt.hist(error, bins=25)\n",
    "plt.xlabel('Prediction Error [Epit]')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSyaHUfDT-mZ"
   },
   "source": [
    "If you're happy with the model you can save it for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-WwLlmfT-mb"
   },
   "outputs": [],
   "source": [
    "dnn_model.save('dnn_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Benlnl8UT-me"
   },
   "source": [
    "If you reload the model, it will give you identical outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyyyj2zVT-mf"
   },
   "outputs": [],
   "source": [
    "reloaded = tf.keras.models.load_model('dnn_model')\n",
    "\n",
    "test_results['reloaded'] = reloaded.evaluate(\n",
    "    X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_GchJ2tg-2o"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(test_results, index=['Mean absolute error [Epit]', 'Root Mean squared error [Epit]', 'R2[Epit]', 'Ratio[Epit]']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgGQuV-yqYZH"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook introduced a few techniques to handle a regression problem. Here are a few more tips that may help:\n",
    "\n",
    "* [Mean Squared Error (MSE)](https://www.tensorflow.org/api_docs/python/tf/losses/MeanSquaredError) and [Mean Absolute Error (MAE)](https://www.tensorflow.org/api_docs/python/tf/losses/MeanAbsoluteError) are common loss functions used for regression problems. Mean Absolute Error is less sensitive to outliers. Different loss functions are used for classification problems.\n",
    "* Similarly, evaluation metrics used for regression differ from classification.\n",
    "* When numeric input data features have values with different ranges, each feature should be scaled independently to the same range.\n",
    "* Overfitting is a common problem for DNN models, it wasn't a problem for this tutorial. See the [overfit and underfit](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit) tutorial for more help with this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, to improve something you often need to be able to measure it. TensorBoard is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualizing the model graph, projecting embeddings to a lower dimensional space, and much more.\n",
    "\n",
    "TensorBoard can be used directly within a notebook in Colab and Jupyter. This can be helpful for sharing results, integrating TensorBoard into existing workflows, and using TensorBoard without installing anything locally.\n",
    "\n",
    "You can find a nice introductory notebook [here](https://www.tensorflow.org/tensorboard/get_started).\n",
    "\n",
    "When you are running this NB in a browster and the tensorboard cannot be displayed: If you are using Safari, try to switch to Google Chrome and run it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example using TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training with Keras's `.fit()`, adding the `tf.keras.callbacks.TensorBoard` callback ensures that logs are created and stored. Additionally, enable histogram computation every epoch with histogram_freq=1 (this is turned off by default).\n",
    "\n",
    "We've saved the logs in a timestamped subdirectory to allow easy selection of different training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start TensorBoard within the notebook using magics. At the end of the command you need to specify the path where the log files are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir=./my_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "regression.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "8aa32b01ff488e379869d54485fe571cd62e468924d402deb61413cca0bc17cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
